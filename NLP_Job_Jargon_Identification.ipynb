{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Jargon Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I analyzed job descriptions from a number of different fields. The thought is that these job descriptions might contain both jargon word and phrases.\n",
    "\n",
    "The challenge here will be to analyze the text of the included job descriptions, but to also compare the words and phrases there with a reference set. In this case, I used Reuters news articles as a background corpus to compare all possible jargon text with.\n",
    "\n",
    "I first read in the text of the job descriptions and then tokenized them. I then took the tokens and compared them to the Reuters as both individual tokens and also as bigrams.\n",
    "\n",
    "This project aims for just term differences, so I reported back the tokens that are only in the job descriptions.\n",
    "\n",
    "The code has been built around using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries for using NLTK\n",
    "\n",
    "import nltk.data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.util import bigrams \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "treebank_tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section I: Pre-process job description text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Read job description from repositories using os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file': 'r1', 'content': \"Dominion Engineering, Inc. (DEI; domeng.com) is a small (~40-person) company that supports the commercial energy industry in the US and abroad with technology, laboratory R&D testing, and consulting. The working environment at DEI is close-knit and professional, but not overly formal. Typical project teams are 2-3 persons working together and in collaboration with the Customer. Newer employees work under the general mentorship of more senior engineers, while still maintaining fairly autonomous roles, roles that may evolve over time to meet emergent needs.\\n\\nOne of DEI’s areas of expertise is degradation of nuclear power plant materials. This position would provide engineering analysis support to DEI project managers and subject matter experts for materials degradation projects and may also be called upon to provide support to other emergent DEI projects. Engineering analysis areas will include corrosion evaluation, fracture mechanics, and microstructural characterization, as well as developing and executing laboratory test programs.\\n\\nDEI is seeking candidates graduating in December 2019 through August 2020, or those with 1-3 years of experience, with a bachelor’s or master’s degree in materials science, materials engineering, or chemical engineering with a solid state focus. Skills/attributes of value are:\\n\\n· Strong analytical skills\\n\\n· Clear written and verbal communication\\n\\n· Coursework, experience, and/or interest in learning about metallic properties and microstructures\\n\\n· Willing to travel up to 10% of the time (e.g., for Customer / industry meetings)\\n\\n· Ability to pass a drug screening test and background security investigation\\n\\n· Authorized to work in the United States\\n\\nCompensation and benefits:\\n\\n· $70,000 to $85,000 annually\\n\\n· Annual 15% retirement plan contribution (no employee contribution required)\\n\\n· Worked overtime (over 40 hrs/wk) is usable as flex time or fully compensated\\n\\n· Paid holidays, vacation, and sick leave\\n\\n· Employee health insurance (fully paid by DEI)\\n\\n· Annual bonus program\\n\\n· Relocation assistance\\n\\nDominion Engineering, Inc. is an Equal Opportunity Employer.\\n\\nJob Type: Full-time\\n\\nSalary: $70,000.00 to $85,000.00 /year\\n\\nEducation:\\n\\nBachelor's (Required)\\nWork authorization:\\n\\nUnited States (Required)\\nAdditional Compensation:\\n\\nBonuses\\nOther forms\\nWork Location:\\n\\nOne location\\nBenefits:\\n\\nHealth insurance\\nRetirement plan\\nPaid time off\\nParental leave\"}\n"
     ]
    }
   ],
   "source": [
    "#dir_base = \"/Users/teacher/repos/s20_ds_nlp/homeworks/homework_1/data/\"\n",
    "dir_base = \"/Users/Winnie/Documents/2020 Spring/NLP/homework_1/data/\"\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename , encoding='utf-8').read()\n",
    "    return input_file_text\n",
    "\n",
    "    \n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    for f in files:\n",
    "        file_text = read_file(join(directory, f))\n",
    "        #print(file_text)\n",
    "        file_texts.append({\"file\":f, \"content\": file_text })\n",
    "    return file_texts\n",
    "    \n",
    "# generate the list that contains all the files and their contents\n",
    "text_corpus = read_directory_files(dir_base)\n",
    "print(text_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Clean text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to clean the text data\n",
    "\n",
    "def process_description(job_description_object):\n",
    "    job_description = job_description_object[\"content\"]\n",
    "    #print(job_description)\n",
    "    \n",
    "    # take the job description text, and tokenize it\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(job_description)\n",
    "    \n",
    "    # convert to lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # remove punctualition from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [word.translate(table) for word in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    \n",
    "    # remove numbers\n",
    "    tokens = [word for word in tokens if not word.isdigit()]\n",
    "    print(tokens[0])  \n",
    "    \n",
    "    return tokens \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Generate single-word tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominion\n",
      "come\n",
      "ana\n",
      "driven\n",
      "overview\n",
      "general\n",
      "unique\n",
      "navaide\n",
      "materials\n",
      "inspired\n",
      "gcssigal\n",
      "love\n",
      "sourcing\n",
      "job\n",
      "new\n"
     ]
    }
   ],
   "source": [
    "# This loop will simply apply the text-preprocessing method to all the job descriptions (single-word tokens)\n",
    "all_job_description_words = []\n",
    "for job_description in text_corpus:\n",
    "    all_job_description_words.extend(process_description(job_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv) Generate bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dominion\n",
      "come\n",
      "ana\n",
      "driven\n",
      "overview\n",
      "general\n",
      "unique\n",
      "navaide\n",
      "materials\n",
      "inspired\n",
      "gcssigal\n",
      "love\n",
      "sourcing\n",
      "job\n",
      "new\n"
     ]
    }
   ],
   "source": [
    "# This loop will simply apply the text preprocessing method to all the job descriptions and generate bigrams tokens\n",
    "all_job_description_bigrams = []\n",
    "for job_description in text_corpus:\n",
    "    all_job_description_bigrams.extend(nltk.bigrams(process_description(job_description)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II: Pre-process Reuters Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the corpus I work from\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Clean Reuters text using the same pre-processing method for the job description text and generate single-word tokens and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(reuters.fileids())\n",
    "\n",
    "all_reuters_words = []\n",
    "all_reuters_bigrams = []\n",
    "\n",
    "# for doc_id in reuters.fileids()[0:25]:  <---- When practising, use this; this will only iterate over the first 25 documents\n",
    "for doc_id in reuters.fileids(): \n",
    "    # this doc_text variable will give a text version of the news article. This could be tokenized.\n",
    "    reuters_text = reuters.open(doc_id).read()\n",
    "    #print(reuters_text)\n",
    "    \n",
    "    # run the same job description processing method\n",
    "    # tokenize the text\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    reuters_tokens = word_tokenize(reuters_text)\n",
    "    # convert to lower case\n",
    "    reuters_tokens = [word.lower() for word in reuters_tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    reuters_stripped = [word.translate(table) for word in reuters_tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    reuters_tokens = [word for word in reuters_stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in reuters_tokens if not word in stop_words]\n",
    "    # remove numbers\n",
    "    reuters_tokens = [word for word in reuters_tokens if not word.isdigit()]\n",
    "    \n",
    "    #print(reuters_tokens[0])\n",
    "    \n",
    "    reuters_bigrams = nltk.bigrams(reuters_tokens)\n",
    "    \n",
    "    # add the output to the all_reuters_words list\n",
    "    all_reuters_words.extend(reuters_tokens)\n",
    "    all_reuters_bigrams.extend(reuters_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section III: Compare two corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Identify single-word jargon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['surfaces', 'ordersmonthly', 'http', 'answered', 'rfi', 'prds', 'suitability', 'interacting', 'familiarity', 'toolsetslanguages']\n"
     ]
    }
   ],
   "source": [
    "# find ways to compare the words in the job descriptions and the reuters text\n",
    "# using Python's set capabilities to intersect things\n",
    "\n",
    "# Token jargon\n",
    "a = set(all_job_description_words)\n",
    "b = set(all_reuters_words)\n",
    "jargon = a.difference(b)\n",
    "print(list(jargon)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Identify bigrams jargon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('degree', 'years'), ('measured', 'team'), ('systems', 'combine'), ('takes', 'win'), ('management', 'leverage'), ('resume', 'workforce'), ('testing', 'consulting'), ('manufacturing', 'metals'), ('drawingscontract', 'documents'), ('project', 'responsibilities')]\n"
     ]
    }
   ],
   "source": [
    "c = set(all_job_description_bigrams)\n",
    "d = set(all_reuters_bigrams)\n",
    "bigrams_jargon = c.difference(d)\n",
    "print(list(bigrams_jargon)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of token jargon is:  291\n",
      "The number of bigrams jargon is: 3196\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of token jargon is: \", len(jargon))\n",
    "print(\"The number of bigrams jargon is:\", len(bigrams_jargon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied six steps to pre-process both the job description word list and the reuters word list. These steps are:\n",
    "1) tokenize the words in the text\n",
    "2) convert token to lower case\n",
    "3) remove punctuations from the words list (i.e. !, ?)\n",
    "4) remove remaining tokens that are not alphabetic (i.e. $, >, <)\n",
    "5) filter out stop words (i.e. \"a\", \"the\", \"this\", \"that\")\n",
    "6) remove numbers in the list (i.e. 7, 8, 1999)\n",
    "\n",
    "After pre-processing the text, I converted the two processed lists to sets. I used python's difference function to identify words that are in the job description words set only but not in the reuters words set. This yields the single-word jargon in the job description text. As for the bigrams jargon, I added the step \"nltk.bigrams()\" to convert the single-word tokens into two-word tokens. \n",
    "\n",
    "The results show that there are 291 single-word jargon and 3196 two-word jargon in the job description text in comparision with the reuters corpus. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
